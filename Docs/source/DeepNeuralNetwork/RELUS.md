Rectified Non-Linear Units (ReLUs)
==================================

It's also called as the "Lazy engineers linear function". ReLUs is a type of activation function which is given by::

    f(x) = max(0,x)

The above function returns ``0`` if ``x`` is negative else it returns ``x``.

see https://en.wikipedia.org/wiki/Rectifier_(neural_networks).